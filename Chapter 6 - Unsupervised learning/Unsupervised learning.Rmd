---
title: "Rmarkdown template"
author: "Thomas Vigié"
output: 
  beamer_presentation: 
    includes:
      in_header: preamble.txt
    keep_tex: yes
classoption: aspectratio=169
urlcolor: blue
linkcolor: SFUblue
always_allow_html: true
---



```{r, echo = FALSE, results = "hide", warning = FALSE, message = FALSE}
list.of.packages <- c("tidyverse","rmarkdown","nycflights13", "lubridate", "crimedata", "Lock5Data", "fivethirtyeight", "stargazer", "ISLR", "randomForest", "rpart", "rpart.plot", "latex2exp", "MASS", "kernlab", "ggdendro", "factoextra", "data.table")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(rmarkdown)
library(nycflights13)
library(lubridate)
library(Lock5Data)
library(crimedata)
library(fivethirtyeight)
library(latex2exp)
library(ISLR)
library(stargazer)
library(MASS)
library(data.table) # pacakge to handle data sets
library(randomForest)
library(kernlab)    # Plots nice SVMs
# library(party)
# library(tree)
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ggdendro)  # dendrograms with ggplot customization options!
library(factoextra) # package for nice kmeans scree plots
```


## Disclaimer

These notes are based on the Book \href{https://www.statlearning.com/}{\textbf{Introduction to Statistical Learning with \textsf{R}}}, by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. Any error is my sole responsibility.

I do not allow this content to be published without my consent.

All rights reserved \textcopyright  2023 Thomas Vigié

## Unsupervised learning
- \textbf{Supervised learning} is well understood because we have a way to check the validity of our methods, thanks to the dependent variable: We are \textbf{supervised}
- \textbf{Unsupervised learning} is more tricky. How can we check if we are on the right path? We are \textbf{not supervised}
- Unsupervised learning is often used as an **observatory data analysis** tool, i.e. as a way to visualize the patterns in the data (correlations between variables, similarities among observations across different features)
- We are going over two different methods corresponding to two different objectives:
    - **Principal components analysis (PCA)**: Visualize high dimensional data using a low-dimensional representation containing as much of the variation of the initial data as possible
    - **Clustering**: Create groups of observations based on some similarity measure

## Outline
\label{outline}

- \hyperlink{principal}{\textbf{Principal components}}
- \hyperlink{clustering}{\textbf{Clustering}}
    - \hyperlink{kmeans}{\textbf{K-means clustering}}
    - \hyperlink{hierarchical}{\textbf{Hierarchical clustering}}
- Suggested reading: Chapter 10 in \href{https://www.statlearning.com/}{\textbf{ISLR}}
    

<!-- ## -->
<!-- \begin{center} -->
<!-- \LARGE{\textbf{Supervised learning}} \label{supervised} -->
<!-- \end{center} -->

<!-- ## Supervised learning -->
<!-- - Supervised problems are the most frequent ones for an economist -->
<!-- - We went over linear regression methods, for either the purpose of **estimation** or **prediction** -->
<!-- - In this lecture, we will focus on **prediction** problems -->
<!-- - So **flexibility** is a major asset for finding the optimal bias-variance trade off -->
<!-- - For regression problems, I will talk about -->
<!--     - **Trees** -->
<!--     - **Neural networks** -->


##
\begin{center}
\LARGE{\textbf{Principal component analysis (PCA)}} \label{principal}
\end{center}

## Principal component analysis
- Generally used to observe high-dimensional data before getting into further analysis
- It summarizes variance and correlation patterns of covariates in a more compact way, by reducing the dimension while keeping the important features
- With $\bm{p}$ variables at hand, \textbf{PCA} looks for $\bm{M \ll p}$ variables $\bm{Z_m}$, the **principal components**, that capture the main features (in particular, variance and correlation) of the original variables:
\[
\bm{Z_m = \sum_{j=1}^{p}\phi_{j,m}X_j}
\]

## Principal component analysis
\[
\bm{Z_m = \sum_{j=1}^{p}\phi_{j,m}X_j}
\]

- First, standardize each variable so they all have mean 0 and standard deviation 1 (as PCA is sensitive to scaling)
- This way, $\bm{Z_m}$ has a mean of 0 and a variance/standard deviation of 1
- PCA finds the coefficients $\bm{\phi_{j,m}, \, \, j = 1,\ldots,p; \, \, \, m = 1,\ldots,M}$ such that the **variance** of the new variable $\bm{z_m}$  is maximized:
\[
\bm{{\underset{\{\phi_{1,1},\ldots,\phi_{p,1} \}}{\max} \frac{1}{n}\sum_{i=1}^{n}z_{i,1}^2 \,\,\,\, \textrm{\textbf{subject to}} \,\, \sum_{j=1}^p\phi_{j,1}^2 = 1}}
\]
- Result: $\bm{z_{1,1},\ldots,z_{n,1}}$ are the **scores** of the first principal component, and $\bm{\left( \phi_{1,1},\ldots,\phi_{p,1} \right) }$ is the **loading vector** associated to the first principal component

## Principal component analysis
- PCA finds the coefficients $\bm{\phi_{j,m}, \, \, j = 1,\ldots,p, \, \, \, m = 1,\ldots,M}$ such that the **variance** of the new variable $\bm{z_m}$  is maximized:
\[
\bm{{\underset{\{\phi_{1,1},\ldots,\phi_{p,1} \}}{\max} \frac{1}{n}\sum_{i=1}^{n}z_{i,1}^2 \,\,\,\, \textrm{\textbf{subject to}} \,\, \sum_{j=1}^p\phi_{j,1}^2 = 1}}
\]
- Result: $\bm{z_{1,1},\ldots,z_{n,1}}$ are the **scores** of the first principal component, and $\bm{\left( \phi_{1,1},\ldots,\phi_{p,1} \right) }$ is the **loading vector** associated to the first principal component
- Then, repeat the same maximization problem to find $\bm{z_2}$. But an extra constraint is added: $\bm{z_2}$ has maximal variance **and** is uncorrelated with $\bm{z_1}$. Repeat until you get to $\bm{z_M}$
- The coefficients $\bm{\phi}$ in each loading vector represent the weight of each variable in that component

## Principal component analysis: Illustration
\scriptsize

```{r, out.width = '80%'}
# Data on crimes for each state of the US, along with other covariates.
# 4 covariates here: "Rape", "Assault", "Murder", and "UrbanPop"
data(USArrests)
states <- row.names(USArrests)
pr.out <- prcomp(USArrests, center = TRUE, scale = TRUE)
summary(pr.out)
pr.out$rotation
```

<!-- ## Principal component analysis: Illustration -->
<!-- \scriptsize -->
<!-- ```{r, echo = FALSE} -->
<!-- pokemon <- read.csv("Pokemon.csv") -->
<!-- pokemon <- pokemon[, -1] # Remove the first column -->
<!-- head(pokemon) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- s <- sample(1:nrow(pokemon), size = 40) -->
<!-- poke <- pokemon[s, -c(1, 2, 3, 11, 12)] -->
<!-- row.names(poke) <- pokemon$name[s] -->
<!-- pr.out <- prcomp(poke, center = TRUE, scale =TRUE) -->
<!-- # summary(pr.out) -->
<!-- pr.out$rotation -->
<!-- ``` -->
## Principal component analysis: Illustration

- In the first principal component, ***Murder***, ***Assault*** and ***Rape*** have a similar magnitude **and** sign: There are positively correlated and contribute relatively equally to the first component: That component "summarizes" serious crime overall
- In the second principal component, ***UrbanPop*** has the highest loading in magnitude: That component "summarizes" the level of urbanization of a state
- Standardization of the data is important. When some features are measured in different units, we might end up in a feature having a very high variance, which will affect the results of the PCA

## Principal component analysis: Biplot
- The main results of PCA can be represented by a biplot
- It shows vectors which are the original covariates. Their coordinates correspond to the loading components of the associated principal components on the right and top axes (in the next slide, the $\bm{ \phi_{j,1} }$ and $\bm{ \phi_{j,2} }$ as we look at PC1 and PC2. $\bm{j}$ corresponds to covariate $\bm{j}$). For instance, $\bm{ \phi_{UrbanPop,1} \simeq -0.28 }$ and $\bm{ \phi_{UrbanPop,2} \simeq -0.87}$.
These vectors can be interpreted in three ways (Rossiter, 2014)
    - **Direction**: The more parallel to a principal component, the more the variable contributes to that PC
    - **Length**: The longer the vector, the more variability of the feature of this variable is represented by the PC. Short vectors are better represented by other PC
    - **Angles between vectors**: Indicates the correlation between the features. The closer they are (the smaller the angle), the more positively correlated they are. A right angle shows no correlation, and opposite angles show negative correlations
- The points are the observations plotted according to the principal components on the graph    


## Principal component analysis: Biplot
\begin{center}
```{r, echo = FALSE, message = FALSE, out.width = '70%' }
biplot (pr.out , scale = 0, choices = c(1, 2)) # Show PC1 and PC2
```
\end{center}

<!-- ## Principal component analysis: Biplot -->
<!-- \begin{center} -->
<!-- ```{r, echo = FALSE, message = FALSE, out.width = '70%' } -->
<!-- library(devtools) -->
<!-- install_github("vqv/ggbiplot") -->
<!-- biplot (pr.out , scale = 0, choices = c(1, 2)) # Show PC1 and PC2 -->
<!-- ``` -->
<!-- \end{center} -->

## Principal component analysis: Illustration
- The closer the vectors, the more correlated the variables (here, ***murder***, ***Assault*** and ***Rape***). But ***Murder*** and ***UrbanPop*** are weakly negatively correlated (angle bigger than 90 degrees)
- Observations located far along a component will feature high values (in magnitude) in the variables most used for that component that go
- Ex: California, Nevada, Florida are low in terms of the first component, so they have high crime values as the first component puts more weight on the crime variables and the states follow the direction of the vectors. For North Dakota, these crime numbers are low as it is far along the first component, but in the opposite direction
- Ex: Mississippi features a low value of urban population (as it is far away from the ***UrbanPop*** vector)
- Ex: Vermont features a low  urban population **and** low crime rates as it is located at the opposite of all the vectors


## Principal component analysis: Takeaways
- PCA is a convenient tool to see what variables are correlated **and** feature the most variance when many covariates are available (particularly convenient for Big Data)
- How to choose the number of components?
- No systematic way (if only we had cross validation... sigh), but one can use the percentage of variance explained by each component. If the first 3 components add up to, say, 90\% of the explained variance, whereas adding a $4^{th}$ component increases that share by 1\%, then 3 components summarize the main features pretty well and could be deemed enough
- It is a standard exploratory tool in statistics and Economics
- Can be used for regression if the number of covariates is very large (**Principal Components Regression**)

##
\begin{center}
\LARGE{\textbf{Clustering}} \label{clustering}
\end{center}

## Clustering
- **Clustering** refers to finding subgroups (clusters) in a data set
- We have access to a sample of size $\bm{n}$ for which we observe $\bm{p}$ **features** $\bm{x_j, \, j = 1,\ldots,p}$  (they are not called "covariates" anymore as there is no $\bm{Y}$)
- Idea: Create homogeneous groups among the observations (i.e. groups with **small within-cluster variation**)
- We can
    - Group observations that are similar according to the **features**
    - Group features that are similar according to the **observations**
- Applications: Marketing (market segmentation: find different types of consumers), medicine (identify groups of patients to understand a disease better), urban Economics (identify groups of houses according to type, value, location, etc)

##
\begin{center}
\LARGE{\textbf{Kmeans clustering}} \label{kmeans}
\end{center}

## K-means clustering

- Assume we want to group observations that are similar according to the **features**
- Let $\bm{C_k, \, k = 1, \ldots,K}$ be cluster $\bm{k}$
- Objective: Obtain $\bm{K}$ clusters that do not overlap, and include all the observations
- Implementation: Minimize the amount of **dissimilarity** between observations of one cluster:
- We will use **Euclidean distance** for dissimilarity. For two observations $\bm{i}$ and $\bm{i^{\prime}}$ belonging to a cluster $\bm{C_k}$, the dissimilarity is defined as
\[
\bm{\sum_{j = 1}^{p}\left( x_{i,j} - x_{i^{\prime},j} \right)^2}
\]

## K-means clustering
\[
\bm{\sum_{j = 1}^{p}\left( x_{i,j} - x_{i^{\prime},j} \right)^2}
\]

- It is the distance between observation $\bm{i}$ and $\bm{j}$ over all the features $\bm{j=1,\ldots,p}$
- Now, take the average of all the pairwise distances in the cluster $\bm{C_k}$ to define
\[
\bm{W(C_k) \equiv \frac{1}{|C_k|} \sum_{i,i^{\prime} \in C_k} \sum_{j = 1}^{p}\left( x_{i,j} - x_{i^{\prime},j} \right)^2}
\]

## K-means clustering
\[
\bm{W(C_k) \equiv \frac{1}{|C_k|} \sum_{i,i^{\prime} \in C_k} \sum_{j = 1}^{p}\left( x_{i,j} - x_{i^{\prime},j} \right)^2}
\]

- We want to minimize dissimilarities for each cluster, so the end goal is to solve:
\[
\bm{{\underset{\{C_1,...,C_K\}} {\min} \sum_{k=1}^{K}W(C_k) = \frac{1}{|C_k|} \underset{\{C_1,...,C_K\}} {\min} \sum_{k=1}^{K}\sum_{i,i^{\prime} \in C_k} \sum_{j = 1}^{p}\left( x_{i,j} - x_{i^{\prime},j} \right)^2}}
\]

## K-means clustering: Algorithm
- It is surprisingly easy to implement in a software

\begin{algorithm*}[K-means clustering]
\begin{itemize}
\item Randomly assign a number from $\bm{1}$ to $\bm{K}$ for each observation. Hence, each observation is randomly assigned a cluster
\item For each cluster:
\begin{itemize}
    \item Compute the \textbf{centroid}, i.e. the vector of averages of the $\bm{p}$ features in the cluster
    \item Assign observations to the cluster where their distance from the centroid is the smallest
\item Keep going until observations are not moved from clusters anymore
\end{itemize}
\end{itemize}
\end{algorithm*}
- Pretty straightforward, right?


<!-- ## K-means clustering in \textbf{\textsf{R}} -->

<!-- \centering -->
<!-- ```{r, echo = FALSE, message = FALSE, out.width = '80%'} -->
<!-- data(NCI60) -->
<!-- # NCI60 -->
<!-- nci.labs <- NCI60$labs -->
<!-- nci.data <- NCI60$data -->
<!-- sd.data <- scale(nci.data) -->
<!-- data.dist <- dist(sd.data) -->
<!-- labels <- c(1:length(nci.labs)) -->

<!-- clusto <- kmeans(x = data.dist, centers = 5) -->

<!-- clusto$cluster -->
<!-- ``` -->

<!-- ```{r, echo = FALSE, out.width = '80%'} -->
<!-- set.seed (2) -->
<!-- n <- 100 -->
<!-- x <- matrix ( rnorm (n * 2) , ncol = 2) -->
<!-- x [1:25 , 1] <- x [1:25 , 1] + 3 -->
<!-- x [1:25 , 2] <- x [1:25 , 2] - 4 -->
<!-- ``` -->

<!-- ```{r, out.width = '80%'} -->
<!-- km.out <- kmeans(x , 3 , nstart = 20) -->
<!-- plot (x , col = (km.out $ cluster + 1), main = "K - Means Clustering Results with K = 2 ", xlab = " ", ylab = " ", pch = 20, cex = 2) -->
<!-- ``` -->

## K-means clustering in \textbf{\textsf{R}}
- To illustrate clustering methods, consider the ***Pokemon*** data set
- Each observation is a pokemon, along with their type, and some statistics: Health points, attack, defense, speed, etc...
- Clustering operates with numerical variables, so we are going to create groups of pokemon according to their specs
- In \textbf{\textsf{R}}, the ***kmeans*** function will achieve that

## K-means clustering in \textbf{\textsf{R}}
\scriptsize
<!-- # ```{r} -->
<!-- # arrests_cluster <- kmeans (x = USArrests, centers = 4 , nstart = 20) -->
<!-- # arrests_cluster$cluster -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # arrests_cluster_reverse <- kmeans (x = t(USArrests), centers = 2 , nstart = 20) -->
<!-- # arrests_cluster_reverse$cluster -->
<!-- # ``` -->

```{r, echo = FALSE}
pokemon <- read.csv("Pokemon.csv")
pokemon <- pokemon[, -1] # Remove the first column
pokemon <- pokemon%>%dplyr::filter(!type1 %in% c("Blastoise", "Graass"))
head(pokemon)
```

## K-means clustering in \textbf{\textsf{R}}
\scriptsize

```{r}
pokemon[, c(4:10)] <- scale(pokemon[, c(4:10)]) # scale the data first
# K-means clustering
pokemon_cluster <- kmeans(x = pokemon[, -c(1, 2, 3, 11, 12)], centers = 5, nstart = 20)
# Code taken from Tyler Harris' post on towardsdatascience.com
kmeans_basic_table <- data.frame(pokemon_cluster$size, pokemon_cluster$centers)
kmeans_basic_df <- data.frame(Cluster = pokemon_cluster$cluster, pokemon)
head(kmeans_basic_df)
```

```{r, echo = FALSE}
pokemon_kclust <- tibble(pokemon = pokemon$name, cluster = pokemon_cluster$cluster)
# head(pokemon_kclust)
```

## K-means clustering in \textbf{\textsf{R}}
\begin{center}
```{r, echo = FALSE, out.width = '100%', fig.width = 20, fig.height = 8, warning = FALSE, message = FALSE}
ggplot(data = kmeans_basic_df, aes(y = Cluster)) +
  geom_bar(aes(fill = type1))  +
  ggtitle("Clusters decomposition by main type") +
  labs(fill = "Main type")  +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(  plot.title = element_text(family = "serif", size = 25),
        axis.title.x = element_text(family = "serif", size = 22),       # Changes fonts into times new roman
        axis.title.y = element_text(family = "serif", size = 22),
        legend.text = element_text(family = "serif", size = 22),
        legend.title = element_text(family = "serif", size = 22))
```
\end{center}

## Choosing the optimal amount of clusters
- There is no one best way, but we can use **scree plots** to see what the somewhat optimal number of clusters should be
- A scree plot relates the number of clusters to the sum of squares withing clusters overall
- The more clusters, the lower the sum of squares
- But we do not want to overfit, in which case one observation has its own cluster
- Where to draw the line depends on us. In general, we choose the number of clusters where the sum of squares does not decrease as fast anymore
- The ***fviz_nbclust*** function from the ***factoextra*** package makes such a plot

## Choosing the optimal amount of clusters
\begin{center}
```{r, echo = FALSE, out.width = '100%', fig.width = 20, fig.height = 8, warning = FALSE, message = FALSE}
# Fancy K-Means
fviz_nbclust(pokemon[, -c(1, 2, 3, 11, 12)], kmeans, nstart = 100, method = "wss")+
theme(  plot.title = element_text(family = "serif", size = 22),
        axis.title.x = element_text(family = "serif", size = 22),       # Changes fonts into times new roman
        axis.title.y = element_text(family = "serif", size = 22),
        legend.text = element_text(family = "serif", size = 22),
        legend.title = element_text(family = "serif", size = 22))
```
\end{center}

## Clustering features
- We can also cluster the features 
- In order to achieve that, the data set needs to be transposed, i.e. rows become columns and columns become rows
- Clusters will be formed according to similarity between features
- In this example: **attack**, **speed**, **hp** etc

## Clustering features
\scriptsize
```{r, echo = FALSE}
names <-  pokemon[, 1]
# Transpose everything other than the first column
poke.T <- as.data.frame(as.matrix(t(pokemon)))
# Assign first column as the column names of the transposed data frame
specs <- rownames(poke.T)
poke.T <- cbind(specs, poke.T)
colnames(poke.T) <- c("specs", names)
poke.T <- poke.T[-1, ]
rownames(poke.T) <- NULL
```

```{r, echo = FALSE}
gna <- kmeans(x = poke.T[-c(1, 2, 3, 10, 11), -1], centers = 3, nstart = 20)
# Code taken from Tyler Harris' post on towardsdatascience.com
kmeans_basic_table <- data.frame(gna$size, gna$centers)
kmeans_basic_df <- data.frame(Cluster = gna$cluster, poke.T[-c(1, 2, 3, 10, 11), ])
# head(kmeans_basic_df)

kmeans_basic_df[1:6, 1:6]
# gna_kclust <- tibble(Spec = poke.T$specs[-c(1, 2, 3, 10, 11)], cluster = gna$cluster)
# head(gna_kclust)
```

##
\begin{center}
\LARGE{\textbf{Hierarchical clustering}} \label{hierarchical}

\end{center}

## Hierarchical clustering

- K-means clustering requires to choose $\bm{K}$, which is no easy task (CV would do if it was a supervised problem...)
- **Hierarchical clustering** produces **dendrograms**, but unlike regression trees in supervised learning, they start at the bottom and go up
- At the bottom, each node has a single observation, i.e. each observation is its own cluster
- Observations are "fused" in order of smallest distance (i.e. smallest **dissimilarity**), one by one
- At the top, there is only one cluster where everybody is, so we need to cut the process before then
- In practice, look at the clusters corresponding to different cuts before choosing where to cut, which is not easy task either (CV would do if it was a supervised problem...)

<!-- ## -->
<!-- - Add more details -->

## Hierarchical clustering: Height of a dendrogram
- Clusters are fused at different heights in the dendrogram
- The height corresponds to the dissimilarity between the corresponding fused clusters
- The higher the clusters are fused, the less similar they are
- We know how to define dissimilarity between two observations (Euclidean distance). But how to compute the dissimilarity between two groups? 
- We need to define the notion of **linkage**. It is based on comparing all pairwise dissimilarities between two clusters. There are four different types:
    - **Complete**: Keep the **largest** of the pairwise dissimilarities
    - **Single**: Keep the **smallest** of the pairwise dissimilarities
    - **Average**: Keep the **average** of the pairwise dissimilarities
    - **Centroid**: Compute the dissimilarity between the centroids of the 2 clusters


<!-- ## Hierarchical clustering in \textbf{\textsf{R}} -->
<!-- \centering -->
<!-- ```{r, echo = FALSE, message = FALSE, out.width = '80%'} -->
<!-- # Hierarchical clustering, complete linkage -->
<!-- # plot(hclust(USArrests, method = "average"), labels = labels , main=" Complete Linkage ", xlab ="", sub ="", ylab ="") -->
<!-- # Hierarchical clustering, average linkage -->
<!-- # plot(hclust (data.dist, method = "average"), labels = labels , main=" Average Linkage ", xlab ="", sub ="", ylab ="") -->
<!-- # Hierarchical clustering, single linkage -->
<!-- # plot(hclust (data.dist, method = "single"), labels = labels , main=" Single Linkage ", xlab ="", sub ="", ylab ="") -->
<!-- ``` -->


<!-- ## Hierarchical clustering in \textbf{\textsf{R}} -->
<!-- \centering -->
<!-- ```{r, echo = FALSE, message = FALSE, out.width = '80%'} -->
<!-- data(NCI60) -->
<!-- # NCI60 -->
<!-- nci.labs <- NCI60$labs -->
<!-- nci.data <- NCI60$data -->
<!-- sd.data <- scale(nci.data) -->
<!-- data.dist <- dist(sd.data) -->
<!-- labels <- c(1:length(nci.labs)) -->
<!-- # Hierarchical clustering, complete linkage -->
<!-- plot(hclust (data.dist, method = "complete"), labels = labels , main=" Complete Linkage ", xlab ="", sub ="", ylab ="") -->
<!-- # Hierarchical clustering, average linkage -->
<!-- # plot(hclust (data.dist, method = "average"), labels = labels , main=" Average Linkage ", xlab ="", sub ="", ylab ="") -->
<!-- # Hierarchical clustering, single linkage -->
<!-- # plot(hclust (data.dist, method = "single"), labels = labels , main=" Single Linkage ", xlab ="", sub ="", ylab ="") -->
<!-- ``` -->

<!-- ## Hierarchical clustering in \textbf{\textsf{R}} -->
<!-- \centering -->
<!-- ```{r, echo = FALSE, message = FALSE, out.width = '80%'} -->
<!-- # Hierarchical clustering, average linkage -->
<!-- plot(hclust (data.dist, method = "average"), labels = labels , main=" Average Linkage ", xlab ="", sub ="", ylab ="") -->
<!-- # Hierarchical clustering, single linkage -->
<!-- # plot(hclust (data.dist, method = "single"), labels = labels , main=" Single Linkage ", xlab ="", sub ="", ylab ="") -->
<!-- ``` -->

<!-- ## Hierarchical clustering in \textbf{\textsf{R}} -->
<!-- \centering -->
<!-- ```{r, echo = FALSE, message = FALSE, out.width = '80%'} -->
<!-- # Hierarchical clustering, single linkage -->
<!-- plot(hclust (data.dist, method = "single"), labels = labels , main=" Single Linkage ", xlab ="", sub ="", ylab ="") -->
<!-- ``` -->



<!-- ```{r} -->
<!-- # Compute distances and hierarchical clustering -->
<!-- dd <- dist(scale(USArrests), method = "euclidean") -->
<!-- hc <- hclust(dd) -->
<!-- plot(hc) -->
<!-- ``` -->

```{r, echo = FALSE}
# Customized dendrograms from https://rpubs.com/TX-YXL/662586
dendro_data_k <- function(hc, k) {
  
  hcdata    <-  ggdendro::dendro_data(hc, type = "rectangle")
  seg       <-  hcdata$segments
  labclust  <-  cutree(hc, k)[hc$order]
  segclust  <-  rep(0L, nrow(seg))
  heights   <-  sort(hc$height, decreasing = TRUE)
  height    <-  mean(c(heights[k], heights[k - 1L]), na.rm = TRUE)
  
  for (i in 1:k) {
    xi      <-  hcdata$labels$x[labclust == i]
    idx1    <-  seg$x    >= min(xi) & seg$x    <= max(xi)
    idx2    <-  seg$xend >= min(xi) & seg$xend <= max(xi)
    idx3    <-  seg$yend < height
    idx     <-  idx1 & idx2 & idx3
    segclust[idx] <- i
  }
  
  idx                    <-  which(segclust == 0L)
  segclust[idx]          <-  segclust[idx + 1L]
  hcdata$segments$clust  <-  segclust
  hcdata$segments$line   <-  as.integer(segclust < 1L)
  hcdata$labels$clust    <-  labclust
  
  hcdata
}
set_labels_params <- function(nbLabels,
                              direction = c("tb", "bt", "lr", "rl"),
                              fan       = FALSE) {
  if (fan) {
    angle       <-  360 / nbLabels * 1:nbLabels + 90
    idx         <-  angle >= 90 & angle <= 270
    angle[idx]  <-  angle[idx] + 180
    hjust       <-  rep(0, nbLabels)
    hjust[idx]  <-  1
  } else {
    angle       <-  rep(0, nbLabels)
    hjust       <-  0
    if (direction %in% c("tb", "bt")) { angle <- angle + 45 }
    if (direction %in% c("tb", "rl")) { hjust <- 1 }
  }
  list(angle = angle, hjust = hjust, vjust = 0.5)
}
plot_ggdendro <- function(hcdata,
                          direction   = c("lr", "rl", "tb", "bt"),
                          fan         = FALSE,
                          scale.color = NULL,
                          branch.size = 1,
                          label.size  = 3,
                          nudge.label = 0.01,
                          expand.y    = 0.1) {
  
  direction <- match.arg(direction) # if fan = FALSE
  ybreaks   <- pretty(segment(hcdata)$y, n = 5)
  ymax      <- max(segment(hcdata)$y)
  
  ## branches
  p <- ggplot() +
    geom_segment(data         =  segment(hcdata),
                 aes(x        =  x,
                     y        =  y,
                     xend     =  xend,
                     yend     =  yend,
                     linetype =  factor(line),
                     colour   =  factor(clust)),
                 lineend      =  "round",
                 show.legend  =  FALSE,
                 size         =  branch.size)
  
  ## orientation
  if (fan) {
    p <- p +
      coord_polar(direction = -1) +
      scale_x_continuous(breaks = NULL,
                         limits = c(0, nrow(label(hcdata)))) +
      scale_y_reverse(breaks = ybreaks)
  } else {
    p <- p + scale_x_continuous(breaks = NULL)
    if (direction %in% c("rl", "lr")) {
      p <- p + coord_flip()
    }
    if (direction %in% c("bt", "lr")) {
      p <- p + scale_y_reverse(breaks = ybreaks)
    } else {
      p <- p + scale_y_continuous(breaks = ybreaks)
      nudge.label <- -(nudge.label)
    }
  }
  
  # labels
  labelParams <- set_labels_params(nrow(hcdata$labels), direction, fan)
  hcdata$labels$angle <- labelParams$angle
  
  p <- p +
    geom_text(data        =  label(hcdata),
              aes(x       =  x,
                  y       =  y,
                  label   =  label,
                  colour  =  factor(clust),
                  angle   =  angle),
              vjust       =  labelParams$vjust,
              hjust       =  labelParams$hjust,
              nudge_y     =  ymax * nudge.label,
              size        =  label.size,
              show.legend =  FALSE)
  
  # colors and limits
  if (!is.null(scale.color)) {
    p <- p + scale_color_manual(values = scale.color)
  }
  
  ylim <- -round(ymax * expand.y, 1)
  p    <- p + expand_limits(y = ylim)
  
  p
}
```
    
        
    
```{r, echo = FALSE}
# hierarchical clustering
# Select some pokemon randomly (too many otherwise)
n <- nrow(pokemon)  
# s <- sample(1:5, size = 40, replace = TRUE)
s1 <- sample(which(pokemon_kclust$cluster==1), 8)
s2 <- sample(which(pokemon_kclust$cluster==2), 8)
s3 <- sample(which(pokemon_kclust$cluster==3), 8)
s4 <- sample(which(pokemon_kclust$cluster==4), 8)
s5 <- sample(which(pokemon_kclust$cluster==5), 8)
s <- c(s1, s2, s3, s4, s5)


# Select the numerical variables
poke <- pokemon[s, -c(1, 2, 3, 11, 12)]
poke <- scale(poke)
row.names(poke) <- pokemon$name[s]
poke <- dist(poke)
poke_hclust <- hclust(poke, method = "single")  # The actual hierarchical clustering function
poke_dendro <- as.dendrogram(poke_hclust)   # Change the format into a dendrogram to plot
# par(mar = c(6.1, 4.1, 4.1, 5.1))
# plot(poke_dendro)

# plot(poke_hclust, labels = labels , main="Single Linkage", xlab ="", sub ="", ylab ="")

# dat <- dendro_data(poke_dendro, type = "rectangle")
# ggplot(segment(dat)) + 
#   geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) + 
  # coord_flip() + 
  # scale_y_reverse(expand = c(0.2, 0))

```

## Hierarchical clustering in \textbf{\textsf{R}}
<!-- \scriptsize -->
\centering

```{r, out.width = '70%', echo = FALSE}
# ggdendrogram(poke_hclust, rotate = TRUE, theme_dendro = FALSE)+
#     theme(  plot.title = element_text(family = "serif"),
#         axis.title.x = element_text(family = "serif"),       # Changes fonts into times new roman
#         axis.title.y = element_text(family = "serif"),
#         legend.text = element_text(family = "serif"),
#         legend.title = element_text(family = "serif"))

hcdata <- dendro_data_k(poke_hclust, 5)
plot_ggdendro(hcdata,
                   direction   = "lr",
                   expand.y    = 0.2) +
  ylab("Dissimilarity")+
  xlab("Pokemon")+
    theme(  plot.title = element_text(family = "serif", size = 22),
        axis.title.x = element_text(family = "serif", size = 22),       # Changes fonts into times new roman
        axis.title.y = element_text(family = "serif", size = 22),
        legend.text = element_text(family = "serif", size = 22),
        legend.title = element_text(family = "serif", size = 22))
```

## Hierarchical clustering in \textbf{\textsf{R}}
\centering
```{r, out.width = '70%', echo = FALSE}
# cols <- c("#a9a9a9", "#1f77b4", "#ff7f0e", "#2ca02c", "blue", "green")
p <- plot_ggdendro(hcdata,
                   fan         = TRUE,
                   # scale.color = cols,
                   label.size  = 4,
                   nudge.label = 0.02,
                   expand.y    = 0.4)   + 
    theme(  plot.title = element_text(family = "serif", size = 22),
        axis.title.x = element_text(family = "serif", size = 22),       # Changes fonts into times new roman
        axis.title.y = element_text(family = "serif", size = 22),
        legend.text = element_text(family = "serif", size = 22),
        legend.title = element_text(family = "serif", size = 22))
p
```
    

## Practical issues with clustering
- As with supervised methods, clustering requires fine tuning of some parameters
- But being unsupervised, it is more difficult to choose an optimal cluster structure, as we do not have access to a criterion function to assess the performance of the structure
- In K-means clustering, how many clusters should we choose?
- In hierarchical clustering, what type of linkage should we use? What measure of dissimilarity? Where should we cut the dendrograms?
- Clustering methods assign **each** observation to a cluster, including outliers, which will make some clusters less meaningful
- More generally, clustering methods are very sensitive to a change in the data, i.e. they organize clusters very differently after removing some subsets of the data. But they can also help detect anomalies (outliers with regards to one particular feature)

## Conclusion
- Unsupervised problems are more tricky due to the absence of supervision
- Many different configurations are acceptable, so make sure you try many to catch all the relevant patterns
- They are usually a **pre-training** analysis (i.e. before estimation/prediction problems)
- Many other methods exist: fuzzy/soft clustering (assigns probabilities of being in each cluster), distribution-based (clusters data points by probability of belonging to the same distribution), density-based (clusters data points based on their concentration)
- They can save a whole analysis as they can help detect strange patterns or outliers, so do not neglect it!